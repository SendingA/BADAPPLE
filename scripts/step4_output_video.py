import os
import gc
import random
from concurrent.futures import ThreadPoolExecutor
from PIL import Image, ImageFilter, ImageDraw, ImageFont
from moviepy.editor import (
    ImageSequenceClip,
    AudioFileClip,
    CompositeVideoClip,
    concatenate_videoclips,
    VideoFileClip,
    vfx,
    TextClip  # æ·»åŠ  TextClip å¯¼å…¥
)
import json
from datetime import datetime
import chardet
import concurrent.futures
from tqdm import tqdm
import numpy as np
import textwrap
import re

def get_config():
    config_file = os.path.join(os.path.dirname(os.path.dirname(os.path.abspath(__file__))), 'config.json')

    with open(config_file, 'rb') as f:
        encoding_result = chardet.detect(raw_data := f.read())
        encoding = encoding_result['encoding']

    return json.loads(raw_data.decode(encoding))

def transform_image(img, t, x_speed, y_speed, move_on_x, move_positive):
    original_size = img.size

    crop_width = img.width * 0.8
    crop_height = img.height * 0.8
    if move_on_x:
        left = min(x_speed * t, img.width - crop_width) if move_positive else max(img.width - crop_width - x_speed * t, 0)
        upper = (img.height - crop_height) / 2
    else:
        upper = min(y_speed * t, img.height - crop_height) if move_positive else max(img.height - crop_height - y_speed * t, 0)
        left = (img.width - crop_width) / 2

    right = left + crop_width
    lower = upper + crop_height

    cropped_img = img.crop((left, upper, right, lower))
    
    return cropped_img.resize(original_size)


def create_subtitle_image(text, width, height, fontsize=36):
    """åˆ›å»ºç®€æ´çš„å­—å¹•å›¾åƒ"""
    img = Image.new('RGBA', (width, height), (0, 0, 0, 0))
    draw = ImageDraw.Draw(img)
    
    try:
        # å°è¯•ä½¿ç”¨æ”¯æŒä¸­æ–‡çš„å­—ä½“
        font = ImageFont.truetype("C:/Windows/Fonts/simhei.ttf", fontsize)
    except:
        try:
            font = ImageFont.truetype("arial.ttf", fontsize)
        except:
            font = ImageFont.load_default()
    
    if not text.strip():
        return np.array(img)
    
    # è·å–æ–‡æœ¬å°ºå¯¸
    bbox = draw.textbbox((0, 0), text, font=font)
    text_width = bbox[2] - bbox[0]
    text_height = bbox[3] - bbox[1]
    
    # ç¡®ä¿å­—å¹•åœ¨å±å¹•èŒƒå›´å†…
    margin = 20
    if text_width > width - 2 * margin:
        # å¦‚æœæ–‡æœ¬å¤ªå®½ï¼Œç¼©å°å­—ä½“
        fontsize = int(fontsize * (width - 2 * margin) / text_width)
        try:
            font = ImageFont.truetype("C:/Windows/Fonts/simhei.ttf", fontsize)
        except:
            font = ImageFont.load_default()
        bbox = draw.textbbox((0, 0), text, font=font)
        text_width = bbox[2] - bbox[0]
        text_height = bbox[3] - bbox[1]
    
    # è®¡ç®—å±…ä¸­ä½ç½®ï¼ˆé è¿‘åº•éƒ¨ï¼‰
    x = (width - text_width) // 2
    y = height - text_height - 80  # è·ç¦»åº•éƒ¨80åƒç´ 
    
    # ç¡®ä¿ä¸ä¼šè¶…å‡ºè¾¹ç•Œ
    x = max(margin, min(x, width - text_width - margin))
    y = max(margin, min(y, height - text_height - margin))
    
    # ç»˜åˆ¶åŠé€æ˜èƒŒæ™¯
    bg_padding = 10
    bg_left = x - bg_padding
    bg_top = y - bg_padding
    bg_right = x + text_width + bg_padding
    bg_bottom = y + text_height + bg_padding
    draw.rectangle([bg_left, bg_top, bg_right, bg_bottom], fill=(0, 0, 0, 128))
    
    # ç»˜åˆ¶æ–‡æœ¬æè¾¹
    for dx in [-1, 0, 1]:
        for dy in [-1, 0, 1]:
            if dx != 0 or dy != 0:
                draw.text((x + dx, y + dy), text, font=font, fill=(0, 0, 0, 255))
    
    # ç»˜åˆ¶ä¸»æ–‡æœ¬
    draw.text((x, y), text, font=font, fill=(255, 255, 255, 255))
    
    return np.array(img)

def load_audio_timing_info(voice_dir):
    """åŠ è½½éŸ³é¢‘æ—¶é•¿ä¿¡æ¯"""
    timing_file = os.path.join(voice_dir, "audio_timing.json")
    sentence_mapping_file = os.path.join(voice_dir, "sentence_mapping.json")
    
    timing_info = {}
    sentence_mapping = {}
    
    if os.path.exists(timing_file):
        with open(timing_file, 'r', encoding='utf-8') as f:
            timing_info = json.load(f)
    
    if os.path.exists(sentence_mapping_file):
        with open(sentence_mapping_file, 'r', encoding='utf-8') as f:
            sentence_mapping_list = json.load(f)
            # è½¬æ¢ä¸ºä»¥åœºæ™¯ç´¢å¼•ä¸ºé”®çš„å­—å…¸
            for item in sentence_mapping_list:
                sentence_mapping[item["scenario_index"]] = item
    
    return timing_info, sentence_mapping

def process_subtitle_ending(subtitle):
    """å¤„ç†å­—å¹•æœ«å°¾ï¼Œç§»é™¤ç»“æŸæ ‡ç‚¹ç¬¦å·"""
    if not subtitle:
        return subtitle
    
    # å®šä¹‰ç»“æŸæ ‡ç‚¹ç¬¦å·
    ending_punctuation = ['ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?', 'ï¼›', ';', 'ï¼Œ', ',']
    
    # å¦‚æœæœ€åä¸€ä¸ªå­—ç¬¦æ˜¯ç»“æŸæ ‡ç‚¹ç¬¦å·ï¼Œåˆ™ç§»é™¤
    while subtitle and subtitle[-1] in ending_punctuation:
        subtitle = subtitle[:-1]
    
    return subtitle.strip()

def create_subtitles_from_audio_timing(scenario_index, timing_info, sentence_mapping, max_chars_per_subtitle=20):
    """æ ¹æ®éŸ³é¢‘æ—¶é•¿ä¿¡æ¯åˆ›å»ºå­—å¹•ï¼Œå¹³å‡åˆ†é…æ˜¾ç¤ºæ—¶é—´"""
    audio_key = f"output_{scenario_index}"
    
    if audio_key not in timing_info or scenario_index not in sentence_mapping:
        return []
    
    sentence_durations = timing_info[audio_key].get("sentence_durations", [])
    processed_sentences = sentence_mapping[scenario_index].get("processed_sentences", [])
    total_duration = timing_info[audio_key].get("total_duration", 0)
    
    if not sentence_durations or not processed_sentences or total_duration <= 0:
        return []
    
    # å°†æ‰€æœ‰å¥å­åˆå¹¶æˆä¸€ä¸ªå®Œæ•´æ–‡æœ¬
    full_text = ''.join(processed_sentences)
    
    # æ™ºèƒ½åˆ†å‰²æ–‡æœ¬ï¼Œè€ƒè™‘åŒå¼•å·çš„æƒ…å†µ
    def smart_split_text(text):
        """æ™ºèƒ½åˆ†å‰²æ–‡æœ¬ï¼Œä¿æŠ¤åŒå¼•å·å†…çš„å†…å®¹"""
        parts = []
        current_part = ""
        in_quotes = False
        quote_chars = ['â€œ', 'â€']  # å„ç§å¼•å·ç±»å‹
        
        i = 0
        while i < len(text):
            char = text[i]
            
            # æ£€æŸ¥æ˜¯å¦é‡åˆ°å¼•å·
            if char in quote_chars and not in_quotes:
                in_quotes = True
                current_part += char
            elif char in quote_chars and in_quotes:
                in_quotes = False
                current_part += char
            # å¦‚æœåœ¨å¼•å·å†…ï¼Œç›´æ¥æ·»åŠ å­—ç¬¦
            elif in_quotes:
                current_part += char
            # å¦‚æœä¸åœ¨å¼•å·å†…ï¼Œæ£€æŸ¥åˆ†å‰²æ ‡ç‚¹
            elif char in ['ï¼Œ', ',', 'ã€‚', 'ï¼', 'ï¼Ÿ', 'ï¼›', ';']:
                current_part += char
                if current_part.strip():
                    parts.append(current_part.strip())
                current_part = ""
            else:
                current_part += char
            
            i += 1
        
        # æ·»åŠ æœ€åä¸€éƒ¨åˆ†
        if current_part.strip():
            parts.append(current_part.strip())
        
        return parts
    
    text_parts = smart_split_text(full_text)
    
    # æ™ºèƒ½åˆå¹¶ï¼šåŒºåˆ†ç»“æŸæ ‡ç‚¹ç¬¦å·å’Œä¸­é—´æ ‡ç‚¹ç¬¦å·
    merged_subtitles = []
    current_subtitle = ""
    
    # å®šä¹‰ç»“æŸæ ‡ç‚¹ç¬¦å·å’Œä¸­é—´æ ‡ç‚¹ç¬¦å·
    ending_punctuation = ['ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?']
    middle_punctuation = ['ï¼Œ', ',', 'ï¼›', ';']
    
    for part in text_parts:
        # æ£€æŸ¥å½“å‰å­—å¹•æ˜¯å¦ä»¥ç»“æŸæ ‡ç‚¹ç¬¦å·ç»“å°¾
        current_ends_with_ending = current_subtitle and current_subtitle[-1] in ending_punctuation
        
        # å¦‚æœå½“å‰å­—å¹•ä»¥ç»“æŸæ ‡ç‚¹ç¬¦å·ç»“å°¾ï¼Œä¸èƒ½åˆå¹¶
        if current_ends_with_ending:
            # ä¿å­˜å½“å‰å­—å¹•
            processed_subtitle = process_subtitle_ending(current_subtitle)
            if processed_subtitle:  # ç¡®ä¿å­—å¹•ä¸ä¸ºç©º
                merged_subtitles.append(processed_subtitle)
            current_subtitle = part
        # å¦‚æœå½“å‰å­—å¹•åŠ ä¸Šæ–°éƒ¨åˆ†ä¸è¶…è¿‡é•¿åº¦é™åˆ¶ï¼Œä¸”å½“å‰å­—å¹•ä¸ä»¥ç»“æŸæ ‡ç‚¹ç¬¦å·ç»“å°¾
        elif len(current_subtitle + part) <= max_chars_per_subtitle:
            current_subtitle += part
        else:
            # ä¿å­˜å½“å‰å­—å¹•ï¼ˆå¦‚æœä¸ä¸ºç©ºï¼‰
            if current_subtitle:
                processed_subtitle = process_subtitle_ending(current_subtitle)
                if processed_subtitle:  # ç¡®ä¿å­—å¹•ä¸ä¸ºç©º
                    merged_subtitles.append(processed_subtitle)
                current_subtitle = ""
            
            # å¦‚æœå•ä¸ªéƒ¨åˆ†å¤ªé•¿ï¼Œéœ€è¦å¼ºåˆ¶æ‹†åˆ†
            if len(part) > max_chars_per_subtitle:
                for i in range(0, len(part), max_chars_per_subtitle):
                    chunk = part[i:i + max_chars_per_subtitle]
                    processed_chunk = process_subtitle_ending(chunk)
                    if processed_chunk:  # ç¡®ä¿å­—å¹•ä¸ä¸ºç©º
                        merged_subtitles.append(processed_chunk)
            else:
                current_subtitle = part
    
    # æ·»åŠ æœ€åçš„å­—å¹•
    if current_subtitle:
        processed_subtitle = process_subtitle_ending(current_subtitle)
        if processed_subtitle:  # ç¡®ä¿å­—å¹•ä¸ä¸ºç©º
            merged_subtitles.append(processed_subtitle)
    
    # å¦‚æœæ²¡æœ‰ç”Ÿæˆä»»ä½•å­—å¹•ï¼Œè¿”å›ç©º
    if not merged_subtitles:
        return []
    
    # å¹³å‡åˆ†é…æ—¶é—´ï¼šæ€»æ—¶é•¿é™¤ä»¥å­—å¹•æ•°é‡
    subtitle_duration = total_duration / len(merged_subtitles)
    
    # åˆ›å»ºå­—å¹•åˆ—è¡¨ï¼Œæ¯ä¸ªå­—å¹•æ˜¾ç¤ºæ—¶é—´ç›¸ç­‰
    subtitles = []
    for i, subtitle_text in enumerate(merged_subtitles):
        start_time = i * subtitle_duration
        end_time = (i + 1) * subtitle_duration
        # ç¡®ä¿æœ€åä¸€ä¸ªå­—å¹•ä¸è¶…è¿‡æ€»æ—¶é•¿
        if i == len(merged_subtitles) - 1:
            end_time = total_duration
        subtitles.append((subtitle_text, start_time, end_time))
    
    return subtitles

def split_text_by_time(text, total_duration, subtitle_duration=2.5, max_chars_per_subtitle=20):
    """æ ¹æ®æ—¶é—´åˆ‡å‰²æ–‡æœ¬ä¸ºçŸ­å­—å¹•ï¼Œå¹³å‡åˆ†é…æ—¶é—´"""
    if not text:
        return []
    
    # ç§»é™¤å¤šä½™çš„ç©ºæ ¼å’Œæ¢è¡Œ
    text = ' '.join(text.split())
    
    # å¦‚æœæ–‡æœ¬å¾ˆçŸ­ï¼Œåªç”¨ä¸€ä¸ªå­—å¹•
    if len(text) <= max_chars_per_subtitle:
        processed_text = process_subtitle_ending(text)
        return [(processed_text, 0, total_duration)]
    
    # æ™ºèƒ½åˆ†å‰²æ–‡æœ¬ï¼Œè€ƒè™‘åŒå¼•å·çš„æƒ…å†µ
    def smart_split_text(text):
        """æ™ºèƒ½åˆ†å‰²æ–‡æœ¬ï¼Œä¿æŠ¤åŒå¼•å·å†…çš„å†…å®¹"""
        parts = []
        current_part = ""
        in_quotes = False
        quote_chars = ['"', '"', '"', "'", "'"]  # å„ç§å¼•å·ç±»å‹
        
        i = 0
        while i < len(text):
            char = text[i]
            
            # æ£€æŸ¥æ˜¯å¦é‡åˆ°å¼•å·
            if char in quote_chars and not in_quotes:
                in_quotes = True
                current_part += char
            elif char in quote_chars and in_quotes:
                in_quotes = False
                current_part += char
            # å¦‚æœåœ¨å¼•å·å†…ï¼Œç›´æ¥æ·»åŠ å­—ç¬¦
            elif in_quotes:
                current_part += char
            # å¦‚æœä¸åœ¨å¼•å·å†…ï¼Œæ£€æŸ¥åˆ†å‰²æ ‡ç‚¹
            elif char in ['ï¼Œ', ',', 'ã€‚', 'ï¼', 'ï¼Ÿ', 'ï¼›', ';']:
                current_part += char
                if current_part.strip():
                    parts.append(current_part.strip())
                current_part = ""
            else:
                current_part += char
            
            i += 1
        
        # æ·»åŠ æœ€åä¸€éƒ¨åˆ†
        if current_part.strip():
            parts.append(current_part.strip())
        
        return parts
    
    text_parts = smart_split_text(text)
    
    # å¦‚æœæ²¡æœ‰åˆ†å‰²å‡ºéƒ¨åˆ†ï¼Œå›é€€åˆ°åŸå§‹æ–‡æœ¬
    if not text_parts:
        text_parts = [text]
    
    # æ™ºèƒ½åˆå¹¶ï¼šåŒºåˆ†ç»“æŸæ ‡ç‚¹ç¬¦å·å’Œä¸­é—´æ ‡ç‚¹ç¬¦å·
    merged_subtitles = []
    current_subtitle = ""
    
    # å®šä¹‰ç»“æŸæ ‡ç‚¹ç¬¦å·å’Œä¸­é—´æ ‡ç‚¹ç¬¦å·
    ending_punctuation = ['ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?']
    middle_punctuation = ['ï¼Œ', ',', 'ï¼›', ';']
    
    for part in text_parts:
        # æ£€æŸ¥å½“å‰å­—å¹•æ˜¯å¦ä»¥ç»“æŸæ ‡ç‚¹ç¬¦å·ç»“å°¾
        current_ends_with_ending = current_subtitle and current_subtitle[-1] in ending_punctuation
        
        # å¦‚æœå½“å‰å­—å¹•ä»¥ç»“æŸæ ‡ç‚¹ç¬¦å·ç»“å°¾ï¼Œä¸èƒ½åˆå¹¶
        if current_ends_with_ending:
            # ä¿å­˜å½“å‰å­—å¹•
            processed_subtitle = process_subtitle_ending(current_subtitle)
            if processed_subtitle:  # ç¡®ä¿å­—å¹•ä¸ä¸ºç©º
                merged_subtitles.append(processed_subtitle)
            current_subtitle = part
        # å¦‚æœå½“å‰å­—å¹•åŠ ä¸Šæ–°éƒ¨åˆ†ä¸è¶…è¿‡é•¿åº¦é™åˆ¶ï¼Œä¸”å½“å‰å­—å¹•ä¸ä»¥ç»“æŸæ ‡ç‚¹ç¬¦å·ç»“å°¾
        elif len(current_subtitle + part) <= max_chars_per_subtitle:
            current_subtitle += part
        else:
            # ä¿å­˜å½“å‰å­—å¹•ï¼ˆå¦‚æœä¸ä¸ºç©ºï¼‰
            if current_subtitle:
                processed_subtitle = process_subtitle_ending(current_subtitle)
                if processed_subtitle:  # ç¡®ä¿å­—å¹•ä¸ä¸ºç©º
                    merged_subtitles.append(processed_subtitle)
                current_subtitle = ""
            
            # å¦‚æœå•ä¸ªéƒ¨åˆ†å¤ªé•¿ï¼Œéœ€è¦å¼ºåˆ¶æ‹†åˆ†
            if len(part) > max_chars_per_subtitle:
                for i in range(0, len(part), max_chars_per_subtitle):
                    chunk = part[i:i + max_chars_per_subtitle]
                    processed_chunk = process_subtitle_ending(chunk)
                    if processed_chunk:  # ç¡®ä¿å­—å¹•ä¸ä¸ºç©º
                        merged_subtitles.append(processed_chunk)
            else:
                current_subtitle = part
    
    # æ·»åŠ æœ€åçš„å­—å¹•
    if current_subtitle:
        processed_subtitle = process_subtitle_ending(current_subtitle)
        if processed_subtitle:  # ç¡®ä¿å­—å¹•ä¸ä¸ºç©º
            merged_subtitles.append(processed_subtitle)
    
    # å¦‚æœæ²¡æœ‰ç”Ÿæˆä»»ä½•å­—å¹•ï¼Œè‡³å°‘æ·»åŠ ä¸€ä¸ª
    if not merged_subtitles:
        processed_text = process_subtitle_ending(text[:max_chars_per_subtitle])
        merged_subtitles = [processed_text]
    
    # å¹³å‡åˆ†é…æ—¶é—´ï¼šæ€»æ—¶é•¿é™¤ä»¥å­—å¹•æ•°é‡
    subtitle_duration_avg = total_duration / len(merged_subtitles)
    
    # åˆ›å»ºå­—å¹•åˆ—è¡¨ï¼Œæ¯ä¸ªå­—å¹•æ˜¾ç¤ºæ—¶é—´ç›¸ç­‰
    subtitles = []
    for i, subtitle_text in enumerate(merged_subtitles):
        start_time = i * subtitle_duration_avg
        end_time = (i + 1) * subtitle_duration_avg
        # ç¡®ä¿æœ€åä¸€ä¸ªå­—å¹•ä¸è¶…è¿‡æ€»æ—¶é•¿
        if i == len(merged_subtitles) - 1:
            end_time = total_duration
        subtitles.append((subtitle_text, start_time, end_time))
    
    return subtitles

def main():

    print("BADAPPLE")

    current_dir = os.path.dirname(os.path.abspath(__file__))
    parent_dir = os.path.dirname(current_dir)

    config = get_config()

    image_dir = os.path.join(parent_dir, 'image')
    voice_dir = os.path.join(parent_dir, 'voice')
    video_dir = os.path.join(parent_dir, 'video')
    temp_dir = os.path.join(parent_dir, 'temp')
    scripts_dir = os.path.join(parent_dir, 'scripts')

    with open(os.path.join(scripts_dir, 'åœºæ™¯åˆ†å‰².json'), 'r', encoding='utf-8') as f:
        scenario_info = list(json.load(f).values())
    os.makedirs(temp_dir, exist_ok=True)

    total_files = len(scenario_info)
    fps = config['fps']
    enlarge_background = config['enlarge_background']
    enable_effect = config['enable_effect']
    effect_type = config['effect_type']

    extensions = ['.png', '.jpg', '.jpeg']
    # åŠ è½½éŸ³é¢‘æ—¶é•¿ä¿¡æ¯
    timing_info, sentence_mapping = load_audio_timing_info(voice_dir)
    # åœ¨ä¸»å¾ªç¯ä¸­ä¿®æ”¹å­—å¹•é•¿åº¦é™åˆ¶
    for i in tqdm(range(total_files), ncols=None, desc="æ­£åœ¨ç”Ÿæˆè§†é¢‘"):
        im_indices = scenario_info[i]['å­å›¾ç´¢å¼•']
        audio_filename = os.path.join(voice_dir, f'output_{i}')
        temp_filename = os.path.join(temp_dir, f'output_{i}.mp4')

        audio = AudioFileClip(audio_filename + '.wav')

        # ä½¿ç”¨éŸ³é¢‘æ—¶é•¿ä¿¡æ¯åˆ›å»ºç²¾ç¡®å¯¹é½çš„å­—å¹•ï¼ˆå¢åŠ å­—å¹•é•¿åº¦ï¼‰
        subtitle_list = create_subtitles_from_audio_timing(i, timing_info, sentence_mapping, max_chars_per_subtitle=23)

        # å¦‚æœæ²¡æœ‰è·å–åˆ°æ—¶é•¿ä¿¡æ¯ï¼Œå›é€€åˆ°åŸæ–¹æ³•ï¼ˆåŒæ ·å¢åŠ å­—å¹•é•¿åº¦ï¼‰
        if not subtitle_list:
            subtitle_text = scenario_info[i].get('å†…å®¹', '')
            subtitle_list = split_text_by_time(subtitle_text, audio.duration, subtitle_duration=2.5, max_chars_per_subtitle=23)

        segment_duration = audio.duration / len(im_indices)
        segment_frames = int(segment_duration * fps)
        all_segments = []

        for idx_num, idx in enumerate(im_indices):
            for ext in extensions:
                try:
                    img_path = os.path.join(image_dir, f'output_{idx+1}{ext}')
                    if os.path.exists(img_path):
                        break
                except FileNotFoundError:
                    print(f"å›¾åƒ output_{idx} æœªæ‰¾åˆ°ï¼Œè·³è¿‡ã€‚")
                    continue

            im = Image.open(img_path)
            effect_type = random.choice([0, 1])

            if effect_type == 0:
                x_speed = (im.width - im.width * 0.8) / segment_duration
                y_speed = 0
                move_on_x = True
            elif effect_type == 1:
                x_speed = 0
                y_speed = (im.height - im.height * 0.8) / segment_duration
                move_on_x = False
            move_positive = random.choice([True, False])
            frames_foreground = [
                np.array(transform_image(im, t / fps, x_speed, y_speed, move_on_x, move_positive)) 
                for t in range(segment_frames)
            ]
            img_foreground = ImageSequenceClip(frames_foreground, fps=fps)

            img_blur = im.filter(ImageFilter.GaussianBlur(radius=30))
            if enlarge_background:
                 img_blur = img_blur.resize(
                    (int(im.width * 1.1), int(im.height * 1.1)), Image.Resampling.LANCZOS)
            frames_background = [np.array(img_blur)] * segment_frames
            img_background = ImageSequenceClip(frames_background, fps=fps)

            segment_clip = CompositeVideoClip(
                [img_background.set_position("center"), img_foreground.set_position("center")], 
                size=img_blur.size
            )

            # ä¸ºå½“å‰æ—¶é—´æ®µæ·»åŠ ç›¸åº”çš„å­—å¹•
            segment_start_time = idx_num * segment_duration
            segment_end_time = (idx_num + 1) * segment_duration

            # æ”¶é›†åœ¨å½“å‰æ—¶é—´æ®µå†…çš„å­—å¹•
            current_subtitles = []
            for subtitle_text, start_time, end_time in subtitle_list:
                # æ£€æŸ¥å­—å¹•æ—¶é—´æ˜¯å¦ä¸å½“å‰æ®µé‡å 
                if not (end_time <= segment_start_time or start_time >= segment_end_time):
                    # è®¡ç®—åœ¨å½“å‰æ®µå†…çš„æ˜¾ç¤ºæ—¶é—´
                    display_start = max(0, start_time - segment_start_time)
                    display_end = min(segment_duration, end_time - segment_start_time)
                    current_subtitles.append((subtitle_text, display_start, display_end))

            # åˆ›å»ºå­—å¹•å›¾å±‚
            if current_subtitles:
                subtitle_clips = []
                for subtitle_text, display_start, display_end in current_subtitles:
                    subtitle_img = create_subtitle_image(subtitle_text, img_blur.size[0], img_blur.size[1])

                    # è®¡ç®—éœ€è¦æ˜¾ç¤ºçš„å¸§æ•°
                    start_frame = int(display_start * fps)
                    end_frame = int(display_end * fps)
                    subtitle_frames = [subtitle_img] * (end_frame - start_frame)

                    if subtitle_frames:
                        subtitle_clip = ImageSequenceClip(subtitle_frames, fps=fps)
                        subtitle_clip = subtitle_clip.set_start(display_start)
                        subtitle_clips.append(subtitle_clip)

                # å¦‚æœæœ‰å­—å¹•ï¼Œæ·»åŠ åˆ°è§†é¢‘ä¸­
                if subtitle_clips:
                    segment_clip = CompositeVideoClip([segment_clip] + subtitle_clips)

            all_segments.append(segment_clip)

        final_clip = concatenate_videoclips(all_segments, method="compose").set_audio(audio)
        final_clip.write_videofile(temp_filename)
        gc.collect()

    temp_filenames = [os.path.join(temp_dir, f'output_{i}.mp4') for i in range(total_files)]
    temp_filenames.sort(key=lambda x: int(x.split('_')[-1].split('.')[0]))
    final_video = concatenate_videoclips([VideoFileClip(filename) for filename in temp_filenames], method="compose")
    final_video.write_videofile(os.path.join(video_dir, f'output_{datetime.now().strftime("%Y%m%d%H%M%S")}.mp4'))
    
if __name__ == "__main__":
    main()
    print("ğŸ‰ è§†é¢‘ç”Ÿæˆå®Œæˆï¼")
